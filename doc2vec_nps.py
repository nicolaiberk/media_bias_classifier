# -*- coding: utf-8 -*-
"""Kopie von hackingCommScience_MediaBias.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ulYM2Cn8cuR9vqQaQnq5_F5kmUSmlC4Z

# Newspaper version of embeddings

Code from https://thinkinfi.com/gensim-doc2vec-python-implementation/
"""

from google.colab import drive
drive.mount('/content/drive')

import gensim
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize

"""Generates a balanced sample of 60,000 articles from 6 newspapers."""

import pandas as pd
import numpy as np

pprs = pd.read_csv("https://www.dropbox.com/s/kc0ylljmp9h9imf/full_sample.csv?dl=1", encoding = "UTF-8", encoding_errors="replace")

pprs = pprs[["date", "text", "label"]]

# Tokenization of each document
pprs = pprs.dropna()
pprs = pprs.reset_index()

doc = pprs.text

tokenized_doc = []
for d in doc:
    tokenized_doc.append(word_tokenize(d.lower()))

# Convert tokenized document into gensim formated tagged data
tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]

## Train doc2vec model
model = Doc2Vec(tagged_data, vector_size=300, window=5, min_count=10, workers=4, epochs = 100)
# Save trained doc2vec model
model.save("drive/MyDrive/bias/nps_doc2vec.model")